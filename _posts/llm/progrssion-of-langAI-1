---
layout: single
title: History / Progression of Language AI Part-1
date: 2025-12-04
categories: [LLM, Fundamentals]
---

## Bag of Words

Bag of Words (BoW) is a technique used for representing unstructured text. It originated in the 1950s but became widely popular in the early 2000s.

It provides a simple way to understand early text processing and how modern LLMs evolved over time.

### How it works

1. **Tokenize** the sentences provided as input.  
   Example:  
   `"Hello, how are you? Are you from India?"`  
   becomes  
   `"Hello"`, `","`, `"how"`, `"are"`, `"you"`, `"?"`, `"Are"`, `"you"`, `"from"`, `"India"`, `"?"`

2. All the **unique tokens** form the **vocabulary**.  
   Example vocabulary:  
   `"Hello"`, `"how"`, `"are"`, `"you"`, `"Are"`, `"from"`, `"India"`, `","`, `"?"`

3. Using the vocabulary, count the **frequency** of each word in the input.  
   This creates a numeric **vector representation** of text.

ğŸ“Œ *Illustration:*  
![Bag of Words](assets/bag_of_words.png)

---

### Limitations

- Space as a delimiter doesnâ€™t work in languages like Mandarin.
- Completely ignores the **meaning** and **context** of words.
- Treats language as just a bag of words â€” order doesnâ€™t matter.

â¡ï¸ This highlights how intelligent humans really are when understanding language ğŸ˜„

---

## Word2Vec

In 2013, **Word2Vec** became one of the first highly successful techniques for capturing **semantics** using **word embeddings** â€” numeric vectors that encode meaning.

It is trained using **large corpora** of text (e.g., Wikipedia) using **neural networks** with weights (parameters) between layers.

ğŸ“Œ *Illustration:*  
![Neural Network](assets/neural_network.png)

---

### How it learns semantic meaning

Word2Vec learns by observing which words frequently appear near each other.

Example sentence:  
> â€œThe cat sat on the mat.â€

- â€œcatâ€ is near â€œsatâ€
- â€œmatâ€ is near â€œsatâ€
- â€œcatâ€ and â€œmatâ€ appear in similar contexts â†’ **similar meanings**

Initially, every word gets a random vector like:

